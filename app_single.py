import gradio as gr
import json
import os
from PIL import Image
import io
import base64
import torch
from diffusers import StableDiffusionImg2ImgPipeline
from google import genai
from google.genai.types import GenerateContentConfig
from dotenv import load_dotenv

# Thi·∫øt l·∫≠p CPU threads
num_cores = os.cpu_count()
torch.set_num_threads(num_cores)
torch.set_num_interop_threads(num_cores)

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# Kh·ªüi t·∫°o Google AI client
client = genai.Client(api_key="AIzaSyAhSbaTFz87GxeS0rc2IFMxhMWKHJMg2YM")

# System prompt cho AI
SYSTEM_PROMPT = """
B·∫°n l√† m·ªôt CHUY√äN GIA T·∫†O PROMPT cho Stable Diffusion img2img v·ªõi chuy√™n m√¥n s√¢u v·ªÅ AI Art Generation.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† chuy·ªÉn ƒë·ªïi y√™u c·∫ßu ƒë∆°n gi·∫£n c·ªßa ng∆∞·ªùi d√πng th√†nh prompt chi ti·∫øt, t·ªëi ∆∞u cho vi·ªác t·∫°o ·∫£nh ch·∫•t l∆∞·ª£ng cao.

### VAI TR√í & CHUY√äN M√îN
- **AI Art Expert**: Hi·ªÉu s√¢u v·ªÅ c√°ch Stable Diffusion ho·∫°t ƒë·ªông v√† t·∫°o ·∫£nh
- **Prompt Engineer**: T·∫°o prompt t·ªëi ∆∞u cho ch·∫•t l∆∞·ª£ng ·∫£nh t·ªët nh·∫•t
- **Technical Specialist**: ƒê·ªÅ xu·∫•t parameters ph√π h·ª£p cho t·ª´ng lo·∫°i chuy·ªÉn ƒë·ªïi

### C·∫§U TR√öC JSON B·∫ÆT BU·ªòC
B·∫°n PH·∫¢I tr·∫£ v·ªÅ ƒë√∫ng c·∫•u tr√∫c JSON n√†y:
{
  "prompt": "",
  "negative_prompt": "",
  "parameters": {
    "steps": 0,
    "cfg_scale": 0,
    "sampler": "",
    "seed": 0
  }
}

### QUY T·∫ÆC T·∫†O PROMPT
1. **prompt**: T·∫°o m√¥ t·∫£ ng·∫Øn g·ªçn (d∆∞·ªõi 60 t·ª´) v·ªÅ ch·ªß th·ªÉ, materials, style, lighting, quality tags t·ªëi ∆∞u nh·∫•t
2. **negative_prompt**: T·ª± ƒë·ªông ch·ªçn c√°c t·ª´ kh√≥a ti√™u c·ª±c ph√π h·ª£p nh·∫•t ƒë·ªÉ tr√°nh l·ªói
3. **parameters**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh c√°c th√¥ng s·ªë t·ªëi ∆∞u cho t·ª´ng lo·∫°i ·∫£nh

### L∆ØU √ù QUAN TR·ªåNG
- Prompt ph·∫£i ng·∫Øn g·ªçn, d∆∞·ªõi 60 t·ª´ ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n token
- ∆Øu ti√™n c√°c t·ª´ kh√≥a quan tr·ªçng nh·∫•t
- Tr√°nh m√¥ t·∫£ qu√° chi ti·∫øt v√† d√†i d√≤ng

### QUY T·∫ÆC B·∫ÆT BU·ªòC
1. **LU√îN b·∫Øt ƒë·∫ßu** v·ªõi m√¥ t·∫£ ch√≠nh x√°c v·ªÅ ƒë·ªëi t∆∞·ª£ng c·∫ßn chuy·ªÉn ƒë·ªïi
2. **Th√™m chi ti·∫øt** v·ªÅ materials, colors, textures c·ª• th·ªÉ
3. **Ch·ªâ ƒë·ªãnh style** ph√π h·ª£p (realistic, artistic, game asset, anime, etc.)
4. **K·∫øt th√∫c** v·ªõi quality tags: "highly detailed, 4K, 8K, masterpiece"
5. **Tr√°nh t·ª´ ng·ªØ m∆° h·ªì**, abstract concepts
6. **KH√îNG d√πng** t·ª´ "like", "similar to", "convert to"
7. **S·ª≠ d·ª•ng ti·∫øng Anh** technical terms cho prompt
8. **Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát** ngo√†i ph·∫ßn JSON

### CH√ö √ù QUAN TR·ªåNG
1. Ch·ªâ hi·ªÉn th·ªã JSON object, kh√¥ng c√≥ text n√†o kh√°c
2. ƒê·∫£m b·∫£o JSON h·ª£p l·ªá v√† ƒë√∫ng c√∫ ph√°p
3. Kh√¥ng th√™m markdown formatting (```json)
""".strip()

class Img2ImgGenerator:
    def __init__(self):
        self.pipe = self.setup_pipeline()
    
    def setup_pipeline(self):
        model_id = "runwayml/stable-diffusion-v1-5"
        
        # Force CPU usage and appropriate dtype
        if torch.cuda.is_available():
            print("GPU available but forcing CPU usage")
        
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float32,  # Use float32 for CPU
            use_safetensors=True,
            device_map=None,  # Prevent automatic device mapping
            low_cpu_mem_usage=True  # Optimize memory usage
        )
        
        # Explicitly move to CPU
        pipe = pipe.to("cpu")
        print("Using CPU with float32 precision")
        
        # Disable safety checker to save memory (optional)
        pipe.safety_checker = None
        pipe.requires_safety_checker = False
        
        # Enable CPU optimizations
        pipe.enable_attention_slicing()
        
        return pipe
    
    def generate(self, input_image_path, prompt, output_path="result.png",
                 strength=0.7, steps=40, guidance=7.5):
        
        # Check if file exists
        if not os.path.exists(input_image_path):
            raise FileNotFoundError(f"Input image not found: {input_image_path}")
        
        # Load image
        print(f"Loading image: {input_image_path}")
        init_image = Image.open(input_image_path).convert("RGB")
        
        # Resize keeping aspect ratio
        init_image = self.resize_image(init_image, 512, 512)
        
        # Generate without autocast for CPU
        print(f"Generating with prompt: {prompt}")
        print("Note: CPU generation will be slower than GPU")
        
        # CPU generation without autocast
        result = self.pipe(
            prompt=prompt,
            image=init_image,
            strength=strength,
            num_inference_steps=steps,
            guidance_scale=guidance,
            height=512,
            width=512
        ).images[0]
        
        # Save
        result.save(output_path)
        print(f"Saved result: {output_path}")
        
        return result
    
    def resize_image(self, image, target_width, target_height):
        # Resize keeping aspect ratio and crop center
        w, h = image.size
        ratio = min(target_width/w, target_height/h)
        new_w, new_h = int(w*ratio), int(h*ratio)
        
        image = image.resize((new_w, new_h), Image.Resampling.LANCZOS)
        
        # Crop center if needed
        left = (new_w - target_width) // 2
        top = (new_h - target_height) // 2
        right = left + target_width
        bottom = top + target_height
        
        if left < 0 or top < 0:
            # Pad if image is smaller than target
            new_image = Image.new('RGB', (target_width, target_height), (255, 255, 255))
            paste_x = (target_width - new_w) // 2
            paste_y = (target_height - new_h) // 2
            new_image.paste(image, (paste_x, paste_y))
            return new_image
        else:
            return image.crop((left, top, right, bottom))

class PromptGenerator:
    def __init__(self):
        pass
    
    def generate_prompt(self, user_request):
        prompt = f"""      
        C√¢u y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng:
        {user_request}
        Tr·∫£ l·ªùi:"""
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt,
            config=GenerateContentConfig(
                system_instruction=[SYSTEM_PROMPT]
            )
        )
        return response.text

# Kh·ªüi t·∫°o generators to√†n c·ª•c - ch·ªâ load 1 l·∫ßn
generator = None
prompt_generator = None

def initialize_generators():
    """Kh·ªüi t·∫°o t·∫•t c·∫£ generators khi app kh·ªüi ƒë·ªông - ch·ªâ load 1 l·∫ßn"""
    global generator, prompt_generator
    try:
        print("üîÑ ƒêang kh·ªüi t·∫°o Stable Diffusion model...")
        generator = Img2ImgGenerator()
        print("‚úÖ ƒê√£ load Stable Diffusion model th√†nh c√¥ng!")
        
        print("üîÑ ƒêang kh·ªüi t·∫°o Prompt Generator...")
        prompt_generator = PromptGenerator()
        print("‚úÖ ƒê√£ kh·ªüi t·∫°o Prompt Generator th√†nh c√¥ng!")
        
        return "‚úÖ ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng t·∫•t c·∫£ models! S·∫µn s√†ng t·∫°o ·∫£nh."
    except Exception as e:
        return f"‚ùå L·ªói kh·ªüi t·∫°o models: {str(e)}"

def process_image_generation(input_image, user_request):
    """X·ª≠ l√Ω t·∫°o ·∫£nh t·ª´ input - s·ª≠ d·ª•ng models ƒë√£ load s·∫µn"""
    global generator, prompt_generator
    
    try:
        # Ki·ªÉm tra models ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
        if generator is None or prompt_generator is None:
            return None, "‚ùå Models ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o! Vui l√≤ng ƒë·ª£i ho·∫∑c refresh trang."
        
        # Ki·ªÉm tra input
        if input_image is None:
            return None, "‚ùå Vui l√≤ng upload ·∫£nh ƒë·∫ßu v√†o!"
        
        if not user_request or user_request.strip() == "":
            return None, "‚ùå Vui l√≤ng nh·∫≠p y√™u c·∫ßu c·ªßa b·∫°n!"
        
        # T·∫°o prompt t·ª´ y√™u c·∫ßu - s·ª≠ d·ª•ng prompt_generator ƒë√£ load s·∫µn
        print("üîÑ ƒêang t·∫°o prompt t·ª´ y√™u c·∫ßu...")
        prompt_data = prompt_generator.generate_prompt(user_request)
        
        # X·ª≠ l√Ω JSON response
        prompt_data = prompt_data.replace("```json", "").replace("```", "").strip()
        prompt_data = json.loads(prompt_data)
        
        print("üìù Prompt ƒë∆∞·ª£c t·∫°o:")
        print(json.dumps(prompt_data, indent=2, ensure_ascii=False))
        
        # L∆∞u ·∫£nh input t·∫°m th·ªùi
        temp_input_path = "temp_input.png"
        input_image.save(temp_input_path)
        
        # T·∫°o ·∫£nh v·ªõi c√°c th√¥ng s·ªë t·ª´ AI - s·ª≠ d·ª•ng generator ƒë√£ load s·∫µn
        print("üé® ƒêang t·∫°o ·∫£nh...")
        print(f"üîß Th√¥ng s·ªë ƒë∆∞·ª£c s·ª≠ d·ª•ng:")
        print(f"   - Steps: {prompt_data['parameters']['steps']}")
        print(f"   - Guidance: {prompt_data['parameters']['cfg_scale']}")
        print(f"   - Strength: 0.8")
        
        result_image = generator.generate(
            input_image_path=temp_input_path,
            prompt=prompt_data["prompt"],
            output_path="temp_output.png",
            strength=0.8,  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh
            steps=prompt_data["parameters"]["steps"],
            guidance=prompt_data["parameters"]["cfg_scale"]
        )
        
        # X√≥a file t·∫°m
        if os.path.exists(temp_input_path):
            os.remove(temp_input_path)
        
        # T·∫°o th√¥ng tin k·∫øt qu·∫£
        result_info = f"""
        ‚úÖ **·∫¢nh ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!**
        
        üìù **Prompt ƒë∆∞·ª£c s·ª≠ d·ª•ng:**
        {prompt_data['prompt']}
        
        üîß **Th√¥ng s·ªë:**
        - Strength: 0.8 (m·∫∑c ƒë·ªãnh)
        - Steps: {prompt_data['parameters']['steps']}
        - Guidance: {prompt_data['parameters']['cfg_scale']}
        
        üéØ **Negative Prompt:**
        {prompt_data.get('negative_prompt', 'Kh√¥ng c√≥')}
        
        üí° **L∆∞u √Ω:** S·ªë b∆∞·ªõc ƒë√£ ƒë∆∞·ª£c AI t·ªëi ∆∞u ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi scheduler
        """
        
        return result_image, result_info
        
    except json.JSONDecodeError as e:
        return None, f"‚ùå L·ªói x·ª≠ l√Ω prompt JSON: {str(e)}"
    except Exception as e:
        return None, f"‚ùå L·ªói khi t·∫°o ·∫£nh: {str(e)}"

def create_interface():
    """T·∫°o giao di·ªán Gradio"""
    
    # CSS t√πy ch·ªânh
    css = """
    .gradio-container {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    .main-header {
        text-align: center;
        color: #2c3e50;
        margin-bottom: 20px;
    }
    .info-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .status-ready {
        color: #27ae60;
        font-weight: bold;
    }
    .status-loading {
        color: #f39c12;
        font-weight: bold;
    }
    """
    
    with gr.Blocks(css=css, title="AI Image Generator - Stable Diffusion") as interface:
        
        # Header
        gr.HTML("""
        <div class="main-header">
            <h1>üé® AI Image Generator</h1>
            <p>Chuy·ªÉn ƒë·ªïi ·∫£nh v·ªõi Stable Diffusion v√† AI Prompt Engineering</p>
        </div>
        """)
        
        # Info box
        gr.HTML("""
        <div class="info-box">
            <h3>üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:</h3>
            <ul>
                <li>Upload ·∫£nh ƒë·∫ßu v√†o b·∫°n mu·ªën chuy·ªÉn ƒë·ªïi</li>
                <li>Nh·∫≠p m√¥ t·∫£ y√™u c·∫ßu b·∫±ng ti·∫øng Vi·ªát (VD: "Chuy·ªÉn th√†nh phong c√°ch anime", "Th√™m hi·ªáu ·ª©ng neon")</li>
                <li>ƒêi·ªÅu ch·ªânh c√°c th√¥ng s·ªë n·∫øu c·∫ßn (ƒë·ªÉ tr·ªëng ƒë·ªÉ d√πng AI t·ª± ƒë·ªông)</li>
                <li>Nh·∫•n "T·∫°o ·∫¢nh" v√† ch·ªù k·∫øt qu·∫£</li>
            </ul>
            <p><strong>üí° T·ªëi ∆∞u:</strong> Models ƒë√£ ƒë∆∞·ª£c load s·∫µn ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω!</p>
        </div>
        """)
        
        with gr.Row():
            with gr.Column(scale=1):
                # Input section
                gr.Markdown("### üì§ Input")
                
                input_image = gr.Image(
                    label="·∫¢nh ƒë·∫ßu v√†o",
                    type="pil",
                    height=300
                )
                
                user_request = gr.Textbox(
                    label="Y√™u c·∫ßu c·ªßa b·∫°n",
                    placeholder="VD: Chuy·ªÉn th√†nh phong c√°ch anime, th√™m hi·ªáu ·ª©ng neon, t·∫°o phi√™n b·∫£n cyberpunk...",
                    lines=3
                )
                
                generate_btn = gr.Button(
                    "üé® T·∫°o ·∫¢nh",
                    variant="primary",
                    size="lg"
                )
                
                # Status
                status_text = gr.Textbox(
                    label="Tr·∫°ng th√°i",
                    value="üîÑ ƒêang kh·ªüi t·∫°o models...",
                    interactive=False
                )
            
            with gr.Column(scale=1):
                # Output section
                gr.Markdown("### üì§ K·∫øt qu·∫£")
                
                output_image = gr.Image(
                    label="·∫¢nh k·∫øt qu·∫£",
                    height=300
                )
                
                output_info = gr.Markdown(
                    label="Th√¥ng tin k·∫øt qu·∫£",
                    value="K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y..."
                )
        
        # Footer
        gr.HTML("""
        <div style="text-align: center; margin-top: 30px; color: #7f8c8d;">
            <p>Powered by Stable Diffusion + Gemini AI | Made with ‚ù§Ô∏è</p>
        </div>
        """)
        
        # Event handlers
        generate_btn.click(
            fn=process_image_generation,
            inputs=[input_image, user_request],
            outputs=[output_image, output_info]
        )
        
        # Auto-initialize generators khi app kh·ªüi ƒë·ªông
        interface.load(initialize_generators, outputs=[status_text])
    
    return interface

# T·∫°o interface
app = create_interface()

# Launch cho Hugging Face Spaces
if __name__ == "__main__":
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        show_error=True
    ) 