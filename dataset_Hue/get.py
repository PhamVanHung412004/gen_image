import requests
from bs4 import BeautifulSoup
import json
import time
import os
from urllib.parse import urljoin, urlparse
import re
from PIL import Image
import hashlib

class HueTourismImageCrawler:
    def __init__(self, output_dir="hue_tourism_data"):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.output_dir = output_dir
        self.images_dir = os.path.join(output_dir, "images")
        self.data = []
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(self.images_dir, exist_ok=True)
    
    def download_image(self, img_url, filename):
        """T·∫£i xu·ªëng h√¨nh ·∫£nh"""
        try:
            response = self.session.get(img_url, timeout=15)
            response.raise_for_status()
            
            # Ki·ªÉm tra k√≠ch th∆∞·ªõc ·∫£nh (ch·ªâ t·∫£i ·∫£nh l·ªõn h∆°n 10KB)
            if len(response.content) < 10240:
                return None
                
            filepath = os.path.join(self.images_dir, filename)
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            return filepath
        except Exception as e:
            print(f"L·ªói t·∫£i ·∫£nh {img_url}: {e}")
            return None
    
    def extract_image_info(self, soup, base_url):
        """Tr√≠ch xu·∫•t th√¥ng tin h√¨nh ·∫£nh v√† m√¥ t·∫£"""
        image_data = []
        
        # T√¨m t·∫•t c·∫£ h√¨nh ·∫£nh
        images = soup.find_all('img')
        
        for idx, img in enumerate(images):
            try:
                # L·∫•y URL ·∫£nh
                img_src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                if not img_src:
                    continue
                
                # Chuy·ªÉn th√†nh URL ƒë·∫ßy ƒë·ªß
                img_url = urljoin(base_url, img_src)
                
                # B·ªè qua ·∫£nh qu√° nh·ªè ho·∫∑c icon
                if any(keyword in img_url.lower() for keyword in ['icon', 'logo', 'avatar', 'thumb']):
                    continue
                
                # L·∫•y th√¥ng tin m√¥ t·∫£ ·∫£nh
                alt_text = img.get('alt', '')
                title_text = img.get('title', '')
                
                # T√¨m caption ho·∫∑c m√¥ t·∫£ g·∫ßn ·∫£nh
                caption = self.find_image_caption(img)
                
                # L·∫•y context xung quanh ·∫£nh
                context = self.get_image_context(img)
                
                # T·∫°o t√™n file duy nh·∫•t
                img_hash = hashlib.md5(img_url.encode()).hexdigest()[:8]
                file_extension = os.path.splitext(urlparse(img_url).path)[1] or '.jpg'
                filename = f"hue_{idx}_{img_hash}{file_extension}"
                
                # T·∫£i ·∫£nh
                local_path = self.download_image(img_url, filename)
                
                if local_path:
                    image_info = {
                        'filename': filename,
                        'local_path': local_path,
                        'original_url': img_url,
                        'alt_text': alt_text,
                        'title': title_text,
                        'caption': caption,
                        'context': context,
                        'source_url': base_url
                    }
                    image_data.append(image_info)
                    print(f"ƒê√£ t·∫£i: {filename}")
                
                time.sleep(1)  # Delay gi·ªØa c√°c l·∫ßn t·∫£i ·∫£nh
                
            except Exception as e:
                print(f"L·ªói x·ª≠ l√Ω ·∫£nh {idx}: {e}")
                continue
        
        return image_data
    
    def find_image_caption(self, img_tag):
        """T√¨m caption c·ªßa ·∫£nh"""
        caption = ""
        
        # T√¨m trong c√°c th·∫ª figure/figcaption
        figure = img_tag.find_parent('figure')
        if figure:
            figcaption = figure.find('figcaption')
            if figcaption:
                caption = figcaption.get_text(strip=True)
        
        # T√¨m trong div parent c√≥ class caption
        parent = img_tag.find_parent(['div', 'span'], class_=re.compile('caption|desc'))
        if parent and not caption:
            caption = parent.get_text(strip=True)
        
        # T√¨m trong sibling elements
        if not caption:
            next_sibling = img_tag.find_next_sibling(['p', 'div', 'span'])
            if next_sibling and len(next_sibling.get_text(strip=True)) < 200:
                caption = next_sibling.get_text(strip=True)
        
        return caption
    
    def get_image_context(self, img_tag):
        """L·∫•y context xung quanh ·∫£nh"""
        context = ""
        
        # T√¨m ƒëo·∫°n vƒÉn tr∆∞·ªõc v√† sau ·∫£nh
        prev_p = img_tag.find_previous('p')
        next_p = img_tag.find_next('p')
        
        if prev_p:
            prev_text = prev_p.get_text(strip=True)
            if len(prev_text) < 300:
                context += prev_text + " "
        
        if next_p:
            next_text = next_p.get_text(strip=True)
            if len(next_text) < 300:
                context += next_text
        
        return context.strip()
    
    def crawl_hue_tourism_sites(self):
        """Crawl c√°c trang du l·ªãch v·ªÅ Hu·∫ø"""
        sites = [
            "https://www.istockphoto.com/vi/b%E1%BB%A9c-%E1%BA%A3nh/hu%E1%BA%BF"
        ]
        
        all_images = []
        
        for site in sites:
            try:
                print(f"\n=== ƒêang crawl: {site} ===")
                response = self.session.get(site, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Tr√≠ch xu·∫•t th√¥ng tin ·∫£nh
                site_images = self.extract_image_info(soup, site)
                all_images.extend(site_images)
                
                print(f"ƒê√£ t√¨m th·∫•y {len(site_images)} ·∫£nh t·ª´ {site}")
                
                time.sleep(3)  # Delay gi·ªØa c√°c site
                
            except Exception as e:
                print(f"L·ªói khi crawl {site}: {e}")
        
        return all_images
    
    def crawl_google_images(self, keywords):
        """T√¨m ·∫£nh t·ª´ Google Images (c·∫©n th·∫≠n v·ªõi Terms of Service)"""
        # L∆∞u √Ω: C·∫ßn tu√¢n th·ªß Terms of Service c·ªßa Google
        search_urls = []
        for keyword in keywords:
            url = f"https://www.google.com/search?q={keyword}&tbm=isch"
            search_urls.append(url)
        
        # Th·ª±c hi·ªán t√¨m ki·∫øm th·∫≠n tr·ªçng
        # (Code n√†y ch·ªâ mang t√≠nh minh h·ªça)
        pass
    
    def save_metadata(self, images_data):
        """L∆∞u metadata c·ªßa t·∫•t c·∫£ ·∫£nh"""
        metadata_file = os.path.join(self.output_dir, "images_metadata.json")
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(images_data, f, ensure_ascii=False, indent=2)
        
        print(f"ƒê√£ l∆∞u metadata v√†o {metadata_file}")
    
    def create_summary_report(self, images_data):
        """T·∫°o b√°o c√°o t·ªïng k·∫øt"""
        report = {
            'total_images': len(images_data),
            'sources': list(set([img['source_url'] for img in images_data])),
            'images_with_captions': len([img for img in images_data if img['caption']]),
            'images_with_context': len([img for img in images_data if img['context']]),
            'summary': []
        }
        
        # T·∫°o t√≥m t·∫Øt cho t·ª´ng ·∫£nh
        for img in images_data:
            summary = {
                'filename': img['filename'],
                'description': img['alt_text'] or img['caption'] or 'Kh√¥ng c√≥ m√¥ t·∫£',
                'has_detailed_info': bool(img['caption'] and img['context'])
            }
            report['summary'].append(summary)
        
        # L∆∞u b√°o c√°o
        report_file = os.path.join(self.output_dir, "crawl_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report
    
    def run(self):
        """Ch·∫°y to√†n b·ªô qu√° tr√¨nh crawl"""
        print("üöÄ B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu h√¨nh ·∫£nh v·ªÅ du l·ªãch Hu·∫ø...")
        
        # Crawl t·ª´ c√°c trang web
        images_data = self.crawl_hue_tourism_sites()
        
        # L∆∞u metadata
        self.save_metadata(images_data)
        
        # T·∫°o b√°o c√°o
        report = self.create_summary_report(images_data)
        
        print(f"\n‚úÖ Ho√†n th√†nh!")
        print(f"üìä T·ªïng c·ªông: {report['total_images']} ·∫£nh")
        print(f"üìù ·∫¢nh c√≥ m√¥ t·∫£: {report['images_with_captions']}")
        print(f"üìã ·∫¢nh c√≥ context: {report['images_with_context']}")
        print(f"üìÅ D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}")

# S·ª≠ d·ª•ng
if __name__ == "__main__":
    crawler = HueTourismImageCrawler()
    crawler.run()
    
    # # C√°c keywords ƒë·ªÉ t√¨m ki·∫øm th√™m (n·∫øu c·∫ßn)
    # hue_keywords = [
    #     "Hu·∫ø cung ƒë√¨nh", "ch√πa Thi√™n M·ª•", "ƒë·∫°i n·ªôi Hu·∫ø", 
    #     "lƒÉng Kh·∫£i ƒê·ªãnh", "lƒÉng T·ª± ƒê·ª©c", "s√¥ng H∆∞∆°ng",
    #     "ch·ª£ ƒê√¥ng Ba", "c·∫ßu Tr∆∞·ªùng Ti·ªÅn"
    # ]